{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUdfwASTXS1j"
      },
      "source": [
        "## Model\n",
        "Definition and demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-26T14:40:15.320131Z",
          "iopub.status.busy": "2023-09-26T14:40:15.319872Z",
          "iopub.status.idle": "2023-09-26T14:40:20.953313Z",
          "shell.execute_reply": "2023-09-26T14:40:20.952266Z",
          "shell.execute_reply.started": "2023-09-26T14:40:15.320106Z"
        },
        "id": "X8pgqpBX1gAQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import timeit\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-26T14:40:20.963686Z",
          "iopub.status.busy": "2023-09-26T14:40:20.958652Z",
          "iopub.status.idle": "2023-09-26T14:40:21.008339Z",
          "shell.execute_reply": "2023-09-26T14:40:21.007489Z",
          "shell.execute_reply.started": "2023-09-26T14:40:20.963650Z"
        },
        "id": "vus3sVPi1gAT"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_CLASSES = 10\n",
        "PATCH_SIZE = 4\n",
        "IMG_SIZE = 28\n",
        "IN_CHANNELS = 1\n",
        "NUM_HEADS = 8\n",
        "DROPOUT = 0.001\n",
        "ADAM_WEIGHT_DECAY = 0\n",
        "ADAM_BETAS = (0.9, 0.999)\n",
        "ACTIVATION=\"gelu\"\n",
        "NUM_ENCODERS = 4\n",
        "EMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS # 16\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2 # 49\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSpYRI4Jtg1D"
      },
      "source": [
        "Fixed code for image with multiple number of channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-26T14:44:18.472373Z",
          "iopub.status.busy": "2023-09-26T14:44:18.471676Z",
          "iopub.status.idle": "2023-09-26T14:44:23.087786Z",
          "shell.execute_reply": "2023-09-26T14:44:23.086684Z",
          "shell.execute_reply.started": "2023-09-26T14:44:18.472339Z"
        },
        "id": "LQKAGDpF1gAU"
      },
      "outputs": [],
      "source": [
        "# Changed dimensions !!!\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
        "        super().__init__()\n",
        "        # Credit: https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/simple_vit.py                                                               \n",
        "        patch_dim = patch_size ** 2 * in_channels\n",
        "        self.patcher = nn.Sequential(\n",
        "            Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1 = patch_size, p2 = patch_size),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(size=(1, 1, embed_dim)), requires_grad=True)\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(size=(1, num_patches+1, embed_dim)), requires_grad=True)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # print('cls_token', self.cls_token.shape)\n",
        "        # print('position_embeddings', self.position_embeddings.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) # (1, 1, embed_dim) --> (Batch_size, 1, embed_dim)\n",
        "        x = self.patcher(x)\n",
        "        x = torch.cat([cls_token, x], dim=1) # (B, NUM_PATCHES, embed_dim ) --> (B, NUM_PATCHES + 1, embed_dim )\n",
        "        # print('x after concat', x.shape)\n",
        "        x = self.position_embeddings + x # (1, NUM_PATCHES + 1, embed_dim ) + (B, NUM_PATCHES + 1, embed_dim ) --> (B, NUM_PATCHES + 1, embed_dim )\n",
        "        x = self.dropout(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LinIrSSqZDnb",
        "outputId": "2f1eef54-23c0-49bc-9b30-988aeb0a12dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 50, 16])\n"
          ]
        }
      ],
      "source": [
        "model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)\n",
        "x = torch.randn(512, 1, 28, 28).to(device)\n",
        "print(model(x).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWaj9uzStnDN"
      },
      "source": [
        "Changed architecture for a stronger model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-09-26T14:44:34.029391Z",
          "iopub.status.busy": "2023-09-26T14:44:34.029018Z",
          "iopub.status.idle": "2023-09-26T14:44:34.122786Z",
          "shell.execute_reply": "2023-09-26T14:44:34.121796Z",
          "shell.execute_reply.started": "2023-09-26T14:44:34.029359Z"
        },
        "id": "soHQGazl1gAU"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, num_patches, img_size, num_classes, patch_size, embed_dim, num_encoders, num_heads, dropout, activation, in_channels):\n",
        "        super().__init__()\n",
        "        self.embeddings_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=4*embed_dim, dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n",
        "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embed_dim),\n",
        "            nn.Linear(in_features=embed_dim, out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings_block(x)\n",
        "        x = self.encoder_blocks(x)\n",
        "        x = self.mlp_head(x[:, 0, :])  # Apply MLP on the CLS token only\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euK6_EkjcwCh",
        "outputId": "78646983-77aa-486a-e45d-3acae72cb95c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 10])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "model = ViT(NUM_PATCHES, IMG_SIZE, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODERS, NUM_HEADS, DROPOUT, ACTIVATION, IN_CHANNELS).to(device)\n",
        "x = torch.randn(512, 1, 28, 28).to(device)\n",
        "print(model(x).shape) # BATCH_SIZE X NUM_CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBirb24JVgEx"
      },
      "source": [
        "## Train model on a downstream task\n",
        "Vì tập MNIST quá đơn giản nên em lấy tập CIFAR100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_-rlNl7fLc0",
        "outputId": "dd4f258a-100f-4543-c82c-4ec83cf1fe16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 100])\n"
          ]
        }
      ],
      "source": [
        "# prompt: import CIFAR-100 from torch\n",
        "RANDOM_SEED = 42\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_CLASSES = 100  # CIFAR-100 has 100 classes\n",
        "PATCH_SIZE = 4\n",
        "IMG_SIZE = 32  # CIFAR-100 images are 32x32\n",
        "IN_CHANNELS = 3  # CIFAR-100 images have 3 color channels\n",
        "NUM_HEADS = 12\n",
        "DROPOUT = 0.1\n",
        "ADAM_WEIGHT_DECAY = 0\n",
        "ADAM_BETAS = (0.9, 0.999)\n",
        "ACTIVATION=\"gelu\"\n",
        "NUM_ENCODERS = 12\n",
        "# EMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS # 48\n",
        "EMBED_DIM = 360\n",
        "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2 # 64\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "model = ViT(NUM_PATCHES, IMG_SIZE, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODERS, NUM_HEADS, DROPOUT, ACTIVATION, IN_CHANNELS).to(device)\n",
        "# model = ViT(EMBED_DIM, NUM_HEADS, DROPOUT, NUM_ENCODERS, NUM_CLASSES, IN_CHANNELS, PATCH_SIZE, IMG_SIZE).to(device)\n",
        "x = torch.randn(BATCH_SIZE, 3, 32, 32).to(device)\n",
        "print(model(x).shape) # BATCH_SIZE X NUM_CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVdA2x9KXX3q",
        "outputId": "44a0c8db-231f-4d91-f914-946c2972cedb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 65, 144])\n"
          ]
        }
      ],
      "source": [
        "patcher = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)\n",
        "x = torch.randn(512, 3, 32, 32).to(device)\n",
        "print(patcher(x).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhhBJSfUfLf-",
        "outputId": "da9a1047-33c6-4000-9956-0e04b11351e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# BTVN:\n",
        "# viết code huấn luyện VIT trên tập MNIST:\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_dataset = CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIq1z9MGcRom",
        "outputId": "bccc59ff-d241-4f2c-f1ec-8561d9fe9185"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/40 - Training: 100%|██████████| 1250/1250 [01:27<00:00, 14.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/40], Train Loss: 4.3339, Train Acc: 3.97%, Val Loss: 3.9881, Val Acc: 8.47%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/40 - Training: 100%|██████████| 1250/1250 [01:29<00:00, 14.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/40], Train Loss: 3.8305, Train Acc: 10.71%, Val Loss: 3.6522, Val Acc: 13.54%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/40 - Training: 100%|██████████| 1250/1250 [01:30<00:00, 13.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/40], Train Loss: 3.5328, Train Acc: 16.25%, Val Loss: 3.4093, Val Acc: 18.17%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/40 - Training: 100%|██████████| 1250/1250 [01:33<00:00, 13.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/40], Train Loss: 3.2944, Train Acc: 20.50%, Val Loss: 3.2580, Val Acc: 20.57%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/40 - Training: 100%|██████████| 1250/1250 [01:30<00:00, 13.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/40], Train Loss: 3.1057, Train Acc: 23.95%, Val Loss: 3.1137, Val Acc: 23.05%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/40 - Training: 100%|██████████| 1250/1250 [01:29<00:00, 13.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/40], Train Loss: 2.9483, Train Acc: 27.10%, Val Loss: 2.9956, Val Acc: 25.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/40 - Training: 100%|██████████| 1250/1250 [01:29<00:00, 13.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/40], Train Loss: 2.8064, Train Acc: 29.49%, Val Loss: 2.9347, Val Acc: 27.10%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/40 - Training:  99%|█████████▉| 1236/1250 [01:28<00:01, 13.93it/s]"
          ]
        }
      ],
      "source": [
        "# prompt: train model on train_loader then evaluate on test_loader. using visualization tools like tensorboard or wandb\n",
        "\n",
        "%pip install wandb -qqq\n",
        "import wandb\n",
        "\n",
        "FLAG = 1\n",
        "\n",
        "# # Initialize wandb\n",
        "if FLAG:\n",
        "  wandb.login()\n",
        "  wandb.init(project=\"ViT-CIFAR100\", entity=\"letangphuquy4-vietnam-korea-university-of-information-an\")  # Replace \"your_wandb_username\" with your actual wandb username\n",
        "  wandb.config.update({\"learning_rate\": LEARNING_RATE, \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE,\n",
        "                      \"num_classes\": NUM_CLASSES, \"patch_size\": PATCH_SIZE, \"img_size\": IMG_SIZE,\n",
        "                      \"in_channels\": IN_CHANNELS, \"num_heads\": NUM_HEADS, \"dropout\": DROPOUT,\n",
        "                      \"adam_weight_decay\": ADAM_WEIGHT_DECAY, \"adam_betas\": ADAM_BETAS,\n",
        "                      \"activation\": ACTIVATION, \"num_encoders\": NUM_ENCODERS, \"embed_dim\": EMBED_DIM,\n",
        "                      \"num_patches\": NUM_PATCHES})\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=ADAM_BETAS, weight_decay=ADAM_WEIGHT_DECAY)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Training\")):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total += target.size(0)\n",
        "        train_correct += (predicted == target).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, target)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Log metrics to wandb\n",
        "    if FLAG:\n",
        "      wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy,\n",
        "                \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], '\n",
        "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_total += target.size(0)\n",
        "        test_correct += (predicted == target).sum().item()\n",
        "\n",
        "test_accuracy = 100 * test_correct / test_total\n",
        "test_loss /= len(test_loader)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%')\n",
        "\n",
        "if FLAG:\n",
        "  # Log test metrics to wandb\n",
        "  wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_accuracy})\n",
        "  # Finish the wandb run\n",
        "  wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
